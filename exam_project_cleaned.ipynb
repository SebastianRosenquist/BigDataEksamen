{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:51.156135Z",
     "iopub.status.busy": "2022-11-15T14:47:51.155947Z",
     "iopub.status.idle": "2022-11-15T14:47:55.689266Z",
     "shell.execute_reply": "2022-11-15T14:47:55.688432Z",
     "shell.execute_reply.started": "2022-11-15T14:47:51.156115Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# loss functions for today\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# stuff for evaluating classifiers\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt # for displaying a pretty confusion matrix\n",
    "\n",
    "\n",
    "# dummy models for comparison\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vectorization involves turning a collection of text documents into a matrix of token counts.\n",
    "\n",
    "In other words, count vectorization means using every observed token (word) across the entire corpus as an attribute (column) and, for each document (row), tallying up how many times each token is observed.\n",
    "\n",
    "We first need to import the data and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:55.690442Z",
     "iopub.status.busy": "2022-11-15T14:47:55.690144Z",
     "iopub.status.idle": "2022-11-15T14:47:55.932479Z",
     "shell.execute_reply": "2022-11-15T14:47:55.931758Z",
     "shell.execute_reply.started": "2022-11-15T14:47:55.690423Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 title  score      id  \\\n0    Subscribe to r/RussiaUKraineWar2022 on Telegra...    473  v0gm37   \n1    10,000 servicemen of the second wave from trai...  14371  y26xch   \n2        r/RussiaUkraineWar2022 Predictions Tournament  18646  ueslps   \n3    I'm safe, fifteen hours with a shovel in my ha...   1519  y39tem   \n4    this is my shell. there are many like it. but ...    339  y3hhep   \n..                                                 ...    ...     ...   \n976  Lieutenant Sergei Didorenko & Senior Lieutenan...    228  xvp4c8   \n977  Ukrainian forces blew up a Russian ammo cache ...    153  xvp2i0   \n978  Ka-52 pilot Captain Aleksey Belonozhko has bee...    304  xvp1gi   \n979      Current frontlines according to Michael McKay    148  xvp0x9   \n980  Russian Commander of a tank company, Negmonov ...    277  xvoz4d   \n\n                subreddit                                                url  \\\n0    RussiaUkraineWar2022                       https://t.me/UkraineWarPosts   \n1    RussiaUkraineWar2022                https://i.redd.it/5e0wl0p78et91.jpg   \n2    RussiaUkraineWar2022  https://reddit.com/r/RussiaUkraineWar2022/pred...   \n3    RussiaUkraineWar2022                https://i.redd.it/m4us0z3gzmt91.jpg   \n4    RussiaUkraineWar2022                https://i.redd.it/t9c4v2iqmot91.jpg   \n..                    ...                                                ...   \n976  RussiaUkraineWar2022              https://www.reddit.com/gallery/xvp4c8   \n977  RussiaUkraineWar2022                    https://v.redd.it/3xd2ekcthur91   \n978  RussiaUkraineWar2022                https://i.redd.it/t6sxzwxlhur91.jpg   \n979  RussiaUkraineWar2022              https://www.reddit.com/gallery/xvp0x9   \n980  RussiaUkraineWar2022                https://i.redd.it/405ja4n7hur91.jpg   \n\n     num_comments                                           body       created  \n0               1                                            NaN  1.653847e+09  \n1             562                                            NaN  1.665589e+09  \n2               1                                            NaN  1.665666e+09  \n3             160  If you have a desire to help me, write to me.  1.665695e+09  \n4              25                                            NaN  1.665714e+09  \n..            ...                                            ...           ...  \n976            20                                            NaN  1.664914e+09  \n977             7                                            NaN  1.664914e+09  \n978            17                                            NaN  1.664914e+09  \n979            17                                            NaN  1.664914e+09  \n980            11                                            NaN  1.664914e+09  \n\n[3792 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>score</th>\n      <th>id</th>\n      <th>subreddit</th>\n      <th>url</th>\n      <th>num_comments</th>\n      <th>body</th>\n      <th>created</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Subscribe to r/RussiaUKraineWar2022 on Telegra...</td>\n      <td>473</td>\n      <td>v0gm37</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://t.me/UkraineWarPosts</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>1.653847e+09</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10,000 servicemen of the second wave from trai...</td>\n      <td>14371</td>\n      <td>y26xch</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://i.redd.it/5e0wl0p78et91.jpg</td>\n      <td>562</td>\n      <td>NaN</td>\n      <td>1.665589e+09</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>r/RussiaUkraineWar2022 Predictions Tournament</td>\n      <td>18646</td>\n      <td>ueslps</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://reddit.com/r/RussiaUkraineWar2022/pred...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>1.665666e+09</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm safe, fifteen hours with a shovel in my ha...</td>\n      <td>1519</td>\n      <td>y39tem</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://i.redd.it/m4us0z3gzmt91.jpg</td>\n      <td>160</td>\n      <td>If you have a desire to help me, write to me.</td>\n      <td>1.665695e+09</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>this is my shell. there are many like it. but ...</td>\n      <td>339</td>\n      <td>y3hhep</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://i.redd.it/t9c4v2iqmot91.jpg</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>1.665714e+09</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>976</th>\n      <td>Lieutenant Sergei Didorenko &amp; Senior Lieutenan...</td>\n      <td>228</td>\n      <td>xvp4c8</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://www.reddit.com/gallery/xvp4c8</td>\n      <td>20</td>\n      <td>NaN</td>\n      <td>1.664914e+09</td>\n    </tr>\n    <tr>\n      <th>977</th>\n      <td>Ukrainian forces blew up a Russian ammo cache ...</td>\n      <td>153</td>\n      <td>xvp2i0</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://v.redd.it/3xd2ekcthur91</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>1.664914e+09</td>\n    </tr>\n    <tr>\n      <th>978</th>\n      <td>Ka-52 pilot Captain Aleksey Belonozhko has bee...</td>\n      <td>304</td>\n      <td>xvp1gi</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://i.redd.it/t6sxzwxlhur91.jpg</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>1.664914e+09</td>\n    </tr>\n    <tr>\n      <th>979</th>\n      <td>Current frontlines according to Michael McKay</td>\n      <td>148</td>\n      <td>xvp0x9</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://www.reddit.com/gallery/xvp0x9</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>1.664914e+09</td>\n    </tr>\n    <tr>\n      <th>980</th>\n      <td>Russian Commander of a tank company, Negmonov ...</td>\n      <td>277</td>\n      <td>xvoz4d</td>\n      <td>RussiaUkraineWar2022</td>\n      <td>https://i.redd.it/405ja4n7hur91.jpg</td>\n      <td>11</td>\n      <td>NaN</td>\n      <td>1.664914e+09</td>\n    </tr>\n  </tbody>\n</table>\n<p>3792 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('./hot_posts.csv')\n",
    "df2 = pd.read_csv('./top_posts.csv')\n",
    "df3 = pd.read_csv('./controversial_posts.csv')\n",
    "df4 = pd.read_csv('./new_posts.csv')\n",
    "dfMerge = [df1, df2, df3, df4]\n",
    "df = pd.concat(dfMerge)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, there are Titles and Scores. We want to perdict if certain words (x) give a high score (Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we Vectorise this data so that the model can read it as 1's and 0's (i.e., a list of all the tokens/words present) and inspect term frequencies.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "CountVectorizer()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(df[\"title\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then print the vocabulary for the vect we created and check for the amount of words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab before post prossesing: 7059.000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vect.vocabulary_)\n",
    "\n",
    "print(\"Number of vocab before post prossesing: {:.3f}\".format(vocab_size))  # 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by removing all low-frequency words. (code adapted from https://stackoverflow.com/questions/57179045/how-to-remove-less-frequent-words-from-pandas-dataframe)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "input_text = df[\"title\"]\n",
    "\n",
    "# Set the threshold for the minimum number of occurrences, in this case 3\n",
    "all_ = [x for y in input_text for x in y.split(' ') ]\n",
    "a, b = np.unique(all_, return_counts = True)\n",
    "to_remove = a[b < 3]\n",
    "\n",
    "# Remove the low-frequency words from the \"title\" column\n",
    "df[\"title\"] = [' '.join(np.array(y.split(' '))[~np.isin(y.split(' '), to_remove)])\n",
    "                for y in input_text]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to clean this list so that our model only has meaningful words to work with.\n",
    "\n",
    "One way of doing this is by <strong style=\"color:red\">removing stopwords</strong>.\n",
    "\n",
    "To do this we can use the ```nltk``` library, which provides a list of stop words for various languages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Seb_R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the list of stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the list of stop words for the English language\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Tokenize the text in the 'text' column\n",
    "df['title'] = df['title'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Define a function to remove stop words from a list of tokens\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Apply the function to the 'text' column of the dataframe\n",
    "df['title'] = df['title'].apply(remove_stop_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We check to see if the code has removed the stop words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    [Telegram, unseen, footage, ., 18+]\n",
      "1      [10,000, servicemen, second, wave, training, U...\n",
      "2      [r/RussiaUkraineWar2022, Predictions, Tournament]\n",
      "3      ['m, safe, ,, fifteen, hours, shovel, hands, l...\n",
      "4                                   [many, like, ., one]\n",
      "                             ...                        \n",
      "976    [Lieutenant, Sergei, &, Senior, Lieutenant, Vl...\n",
      "977    [Ukrainian, forces, blew, Russian, ammo, back,...\n",
      "978    [Ka-52, pilot, Captain, Aleksey, killed, Ukrai...\n",
      "979            [Current, frontlines, according, Michael]\n",
      "980    [Russian, Commander, tank, company, ,, neutral...\n",
      "Name: title, Length: 3792, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['title'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, code has removed the stopwords from our dataframe.\n",
    "\n",
    "We can now focus on removing low-frequency words to help simplify our dataframe for our model and rerun the count vectorizer to continue building our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer\n\u001B[0;32m      3\u001B[0m vect \u001B[38;5;241m=\u001B[39m CountVectorizer()\n\u001B[1;32m----> 4\u001B[0m \u001B[43mvect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1291\u001B[0m, in \u001B[0;36mCountVectorizer.fit\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001B[39;00m\n\u001B[0;32m   1276\u001B[0m \n\u001B[0;32m   1277\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1288\u001B[0m \u001B[38;5;124;03m    Fitted vectorizer.\u001B[39;00m\n\u001B[0;32m   1289\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1290\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warn_for_unused_params()\n\u001B[1;32m-> 1291\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1292\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1330\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1331\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1332\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1333\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1334\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1335\u001B[0m             )\n\u001B[0;32m   1336\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1338\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1341\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1207\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1208\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1209\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1210\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1211\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m preprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 111\u001B[0m         doc \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    113\u001B[0m         doc \u001B[38;5;241m=\u001B[39m tokenizer(doc)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001B[0m, in \u001B[0;36m_preprocess\u001B[1;34m(doc, accent_function, lower)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;124;03mapply to a document.\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;124;03m    preprocessed string\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lower:\n\u001B[1;32m---> 69\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m()\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m accent_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     71\u001B[0m     doc \u001B[38;5;241m=\u001B[39m accent_function(doc)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(df['title'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This returns an error because an array of strings is what the CountVectorizer expects. Therefore, it will crash if we pass in a nested array of tokens. We therefore need to transform the dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "CountVectorizer()",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"] = df[\"title\"].map(' '.join)\n",
    "vect = CountVectorizer()\n",
    "vect.fit(df[\"title\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab before stop-word after post processing: 2177.000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vect.vocabulary_)\n",
    "\n",
    "print(\"Number of vocab before stop-word after post processing: {:.3f}\".format(vocab_size))  # 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This means that we have removed a lot of words to make our model simplere and hopefully more precise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform this data to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.086981Z",
     "iopub.status.busy": "2022-11-15T14:47:56.086777Z",
     "iopub.status.idle": "2022-11-15T14:47:56.219035Z",
     "shell.execute_reply": "2022-11-15T14:47:56.218251Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.086963Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Title_text = df[\"title\"]\n",
    "\n",
    "vector = vect.transform(Title_text)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.220045Z",
     "iopub.status.busy": "2022-11-15T14:47:56.219829Z",
     "iopub.status.idle": "2022-11-15T14:47:56.260681Z",
     "shell.execute_reply": "2022-11-15T14:47:56.259930Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.220020Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [1, 1, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.261717Z",
     "iopub.status.busy": "2022-11-15T14:47:56.261509Z",
     "iopub.status.idle": "2022-11-15T14:47:56.304860Z",
     "shell.execute_reply": "2022-11-15T14:47:56.304047Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.261699Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "array = vector.toarray()[0]\n",
    "inv_voc = {v: k for k, v in vect.vocabulary_.items()}\n",
    "print([inv_voc[x] for x in np.where(array > 1)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create the test data and split it into text_test and text_train. We are only looking at the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.305862Z",
     "iopub.status.busy": "2022-11-15T14:47:56.305662Z",
     "iopub.status.idle": "2022-11-15T14:47:56.311923Z",
     "shell.execute_reply": "2022-11-15T14:47:56.311107Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.305844Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0                    Telegram unseen footage . 18+\n0    r/RussiaUkraineWar2022 Predictions Tournament\n0                           video Mariupol today .\n0       Zelensky offers Russian soldiers surrender\nName: title, dtype: object"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Title_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.312824Z",
     "iopub.status.busy": "2022-11-15T14:47:56.312640Z",
     "iopub.status.idle": "2022-11-15T14:47:56.363907Z",
     "shell.execute_reply": "2022-11-15T14:47:56.363067Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.312807Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = vector.toarray()\n",
    "y = df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.365053Z",
     "iopub.status.busy": "2022-11-15T14:47:56.364847Z",
     "iopub.status.idle": "2022-11-15T14:47:56.370100Z",
     "shell.execute_reply": "2022-11-15T14:47:56.369511Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.365035Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.370933Z",
     "iopub.status.busy": "2022-11-15T14:47:56.370760Z",
     "iopub.status.idle": "2022-11-15T14:47:56.488213Z",
     "shell.execute_reply": "2022-11-15T14:47:56.487402Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.370918Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11122)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then check to see if the X_train and X_test data contains the same amount of columns but with different amounts of rows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.489238Z",
     "iopub.status.busy": "2022-11-15T14:47:56.489025Z",
     "iopub.status.idle": "2022-11-15T14:47:56.494828Z",
     "shell.execute_reply": "2022-11-15T14:47:56.494237Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.489221Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both X_test and X_train have the same amount of columns - 2,177"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.521769Z",
     "iopub.status.busy": "2022-11-15T14:47:56.521551Z",
     "iopub.status.idle": "2022-11-15T14:47:56.548314Z",
     "shell.execute_reply": "2022-11-15T14:47:56.547289Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.521749Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2      7763\n104     223\n641     981\n894     870\n25       90\n       ... \n323    1423\n336     213\n849      66\n130     107\n92        0\nName: score, Length: 2844, dtype: int64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-11-15T14:47:56.550829Z",
     "iopub.status.busy": "2022-11-15T14:47:56.549973Z",
     "iopub.status.idle": "2022-11-15T14:57:35.627784Z",
     "shell.execute_reply": "2022-11-15T14:57:35.626764Z",
     "shell.execute_reply.started": "2022-11-15T14:47:56.550790Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ranreg = RandomForestRegressor()\n",
    "ranreg.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(ranreg.score(X_train, y_train)))\n",
    "print(\"Test score: {:.5f}\".format(ranreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-15T14:57:35.629393Z",
     "iopub.status.busy": "2022-11-15T14:57:35.629175Z",
     "iopub.status.idle": "2022-11-15T14:57:49.181547Z",
     "shell.execute_reply": "2022-11-15T14:57:49.179242Z",
     "shell.execute_reply.started": "2022-11-15T14:57:35.629374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(linreg.score(X_train, y_train)))\n",
    "print(\"Test score: {:.5f}\".format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-15T14:57:49.185076Z",
     "iopub.status.busy": "2022-11-15T14:57:49.184646Z",
     "iopub.status.idle": "2022-11-15T14:57:49.984863Z",
     "shell.execute_reply": "2022-11-15T14:57:49.983552Z",
     "shell.execute_reply.started": "2022-11-15T14:57:49.185037Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kreg = KNeighborsRegressor()\n",
    "kreg.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(kreg.score(X_train, y_train)))\n",
    "print(\"Test score: {:.5f}\".format(kreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "By looking at the 3 results above, we see that the <strong style=\"color:red\">Random Forrest Regressor</strong> yields the most accurate results but that KNeighbor Regression had the largest increase in accuracy from the preprocessing. We will try to make both more accurate.\n",
    "\n",
    "However, both are still very inaccurate and over-fits on the training set. We therefore have to do some more data treatment with our model to get a better more accurate result.\n",
    "\n",
    "This can be done in several ways:\n",
    "1. Increasing the number of trees in the forest (provided that the model is not already overfitting)\n",
    "2. Tuning the hyperparameters of the individual trees, such as the maximum depth of the trees and the minimum number of samples required to split a node\n",
    "3. Using a better quality training dataset that has more relevant features and less noise\n",
    "4. Using cross-validation to evaluate the model and select the best performing set of hyperparameters\n",
    "5. Ensembling multiple random forest models with different parameters to improve the overall accuracy of the model.\n",
    "\n",
    "We know that the model is overfitting already, so increasing the number of trees in the forest, will most likely not improve the accuracy.\n",
    "\n",
    "Therefore, we will start by tweaking the hyperparameters to get a more accurate result.\n",
    "To tune the hyperparameters of our regression model, we will use the RandomizedSearchCV class from the scikit-learn library. This class allows us to define a range of hyperparameters to search over, and it will automatically evaluate a random sampling of combinations of these hyperparameters to find the best performing set of parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter space to search over\n",
    "param_distributions = {\n",
    "    'max_depth': randint(1, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'n_estimators': randint(10, 100),\n",
    "}\n",
    "\n",
    "# Define the random forest regressor\n",
    "regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the search method\n",
    "search = RandomizedSearchCV(regressor, param_distributions, cv=5, random_state=42)\n",
    "\n",
    "# Perform the search\n",
    "search.fit(X, y)\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print('Best hyperparameters:', search.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what hyperparameters that best fit our data, we'll use this to try and improve the results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create an instance of the RandomForestRegressor class\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Set the hyperparameters of the model\n",
    "model.n_estimators = 50  # Number of trees in the forest\n",
    "model.max_depth = 30  # Maximum depth of the tree\n",
    "model.min_samples_split = 10  # Minimum number of samples required to split a node\n",
    "model.min_samples_leaf = 1\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Training set score: {:.3f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score: {:.5f}\".format(model.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll also search on the best parameters on KNeightborsRegression since it had the largest improvement on the before and after the data preprocessing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a KNN Regressor model\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(grid_search.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of the RandomForestRegressor class\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Set the hyperparameters of the model\n",
    "model.n_neighbors = 9  # Number of trees in the forest\n",
    "model.p = 2  # Maximum depth of the tree\n",
    "model.weights = 'distance'  # Minimum number of samples required to split a node\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Training set score: {:.3f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Test score: {:.5f}\".format(model.score(X_test, y_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
